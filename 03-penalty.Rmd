
# Nonconvex Penalty

## Introduction

<p style="text-align:justify;">
In addition to lasso, ridge and elastic net penalty, **sgdnet** also fits along regularization paths for linear regression using nonconvex penalties, which are the minimax concave penalty (MCP) [@zhang2010] and smoothly clipped absolute penalty (SCAD) [@fan2001]. </p>

<p style="text-align:justify;">
The ridge part in **sgdnet** use scale update $\beta^{k + n} = \beta^{k} \prod _{i = 1}^{n}(1- \eta(1-\alpha) \lambda)$, where $\lambda$ is the regularization strength, $\alpha$ is elastic net mixing parameter and $\eta$ is the step size. Other penalties are applied via a proximal step at each iteration from the coefficient $\beta^{k+\frac{1}{2}}$ after the average gradient step. The proximal step solves a minimization problem:
</p>

<center>
$\begin{aligned} \beta^{k + \frac{1}{2}}  \leftarrow & \, \beta^{k} - \eta \nabla \\ \beta^{k} \leftarrow & \, \underset{\beta}{argmin} \Big( h_{\lambda}(\beta) + \frac{1}{2 \eta} \left \| \beta - \beta^{k + \frac{1}{2}} \right \|_2^2 \Big) \end{aligned}$
</center>

where $h_{\lambda}(\beta)$ is the penalty function.

## MCP

<p style="text-align:justify;">
The definition for minimax concave penalty for each element of coefficient $\beta$ is given by:
</p>

$$\begin{equation} p_{\lambda, \gamma}(\beta_j) = \begin{cases}
\lambda \beta_j - \frac{\beta_j^2}{2 \gamma} & \text{if} \,\, \beta_j \leq \gamma \lambda \\ \frac{1}{2} \gamma \lambda^2 & \text{otherwise}
\end{cases}\end{equation}$$

<p style="text-align:justify;">
where $\lambda \geq 0$ is the regularization strength and $\gamma > 1$ is a user defined nonconvexity parameter. We have $h_{\lambda}(\beta) = p_{\alpha \lambda, \gamma}(\beta) + \frac{(1 - \alpha)\lambda}{2} ||\beta||_2^2$ if user wants to mix ridge with MCP in the proximal step by setting $\alpha$ [@huang2016]. The proximal operator for them is given by:
</p>

$$\begin{equation} \beta_j \leftarrow \begin{cases} \frac{ \Big (1 - \eta \alpha \lambda \Big )_{+} \beta_j}{1 + \eta (1-\alpha)\lambda-\frac{\eta}{\gamma}} & \text{if} \,\, \beta_j \leq \gamma \alpha \lambda \\ \frac{\beta_j}{1+\eta(1-\alpha)\lambda} & \text{otherwise}. \end{cases} \end{equation}$$

<p style="text-align:justify;">
MCP begins by applying the same rate of penalization as lasso, but continuously relaxes that penalization until $\beta_j$ becomes large [@breheny2011]. When $\alpha = 0$, this becomes the proximal updates for ridge [@friedman2010]. For MCP, **sgdnet** set the default noncovexity parameter $\gamma$ to $3$. Here we use the example from [**ncvreg**](https://cran.r-project.org/web/packages/ncvreg/index.html) to demostrate its usage.
</p>

```{r ncvregfit, echo = FALSE, message = FALSE}
library(ncvreg)
data(Prostate)
x <- Prostate$X
y <- Prostate$y
ncv_mcp  <- ncvreg(x, y, penalty = "MCP")
ncv_mnet <- ncvreg(x, y, alpha = 0.5, penalty = "MCP")
ncv_scad <- ncvreg(x, y, penalty = "SCAD")
ncv_snet <- ncvreg(x, y, alpha = 0.5, penalty = "SCAD")
detach(package:ncvreg)
```

```{r mcpsetup, echo = FALSE, message = FALSE}
devtools::load_all("~/sgdnet/penalty/sgdnet")
sgd_mcp  <- sgdnet(x, y, penalty = "MCP", lambda = ncv_mcp$lambda)
sgd_mnet <- sgdnet(x, y, alpha = 0.5, penalty = "MCP", 
                   lambda = ncv_mnet$lambda)
sgd_scad <- sgdnet(x, y, penalty = "SCAD", lambda = ncv_scad$lambda)
sgd_snet <- sgdnet(x, y, alpha = 0.5, penalty = "SCAD",
                   lambda = ncv_snet$lambda)
```

```{r mcp, eval=FALSE}
data(prostate)
x <- prostate$x
y <- prostate$y
sgd_mcp  <- sgdnet(x, y, penalty = "MCP")
ncv_mcp  <- ncvreg(x, y, penalty = "MCP")
sgd_mnet <- sgdnet(x, y, alpha = 0.5, penalty = "MCP")
ncv_mnet <- ncvreg(x, y, alpha = 0.5, penalty = "MCP")
```

```{r plotmcp, out.width="50%", out.extra='style="display: inline-block;"', echo=FALSE}
plot(sgd_mcp, xvar = "lambda")
plot(ncv_mcp, log.l = TRUE)
plot(sgd_mnet, xvar = "lambda")
plot(ncv_mnet, log.l = TRUE)
```

## SCAD

<p style="text-align:justify;">
Following the same notation, the definition for smoothly clipped absolute penalty for each element of coefficient $\beta$ is given by:
</p>

$$\begin{equation} p_{\lambda, \gamma}(\beta_j) = \begin{cases} \lambda \beta_j & \text{if} \, \,\beta_j \leq \lambda\\ \frac{\gamma \lambda \beta_j - \frac{1}{2} (\beta_j^2 + \lambda^2)}{\gamma - 1} \gamma \lambda^2 & \text{if} \,\, \lambda < \beta_j \leq \gamma \lambda \\ \frac{\lambda^2 (\gamma^2 - 1)}{2(\gamma - 1)} & \text{therwise} \end{cases}  \end{equation}$$

<p style="text-align:justify;">
where $\lambda \geq 0$ and $\gamma > 2$. The proximal operator for SCAD [@zhu2016] of strength $\alpha \lambda$, with ridge of strength $(1-\alpha)\lambda$ can be solved as:
</p>

$$\begin{equation} \beta_j \leftarrow \begin{cases} 
\frac{\Big ( 1-\eta \alpha \lambda \Big )_{+} \beta_j}{1+\eta(1-\alpha)\lambda} & \text{if} \, \,\beta_j \leq \alpha \lambda + \eta \alpha \lambda ( 1 + (1-\alpha) \lambda)\\ 
\frac{\Big ( 1 - \frac{\eta \gamma}{ \gamma - 1} \alpha \lambda \Big)_{+} \beta_j}{1- \frac{\eta}{\gamma - 1} + \eta (1- \alpha) \lambda} & \text{if} \,\, \alpha \lambda + \eta \alpha \lambda ( 1 + (1-\alpha) \lambda) < \beta_j \leq \gamma \alpha \lambda ( 1 - \eta (1 - \alpha) \lambda) \\ 
\frac{\beta_j}{1+\eta(1-\alpha)\lambda} & \text{therwise} \end{cases}  \end{equation}$$

<p style="text-align:justify;">
Similar to MCP, SCAD begins by applying lasso's penalization rate, and reduces the rate to 0 as $\beta_j$ gets away from 0. The difference is in the way to make this transition. **sgdnet** has a default nonconvexity parameter $\gamma = 3.7$. It can be specified by the ```non_convexity``` variable in ```sgdnet( )``` function.
</p>

```{r scad, eval=FALSE}
sgd_scad <- sgdnet(x, y, penalty = "SCAD")
ncv_scad <- ncvreg(x, y, penalty = "SCAD")
sgd_snet <- sgdnet(x, y, alpha = 0.5, penalty = "SCAD")
ncv_snet <- ncvreg(x, y, alpha = 0.5, penalty = "SCAD")
```

```{r plotscad, out.width="50%", out.extra='style="display: inline-block;"', echo=FALSE}
plot(sgd_scad, xvar = "lambda")
plot(ncv_mcp, log.l = TRUE)
plot(sgd_snet, xvar = "lambda")
plot(ncv_mnet, log.l = TRUE)
```


