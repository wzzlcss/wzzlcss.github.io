[
["index.html", "GSoC 2019 Final Report 1 Overview", " GSoC 2019 Final Report Qincheng Lu 2019-08-25 1 Overview Project title sgdnet: Efficient Regularized GLMs for Big Data Student Qincheng Lu Mentors Michael Weylandt, Johan Larsson and Toby Dylan Hocking Contribution Mini-batch and Cyclic SAGA Observation Weights Optimal Batch Size and Step Size Nonconvex Penalty: MCP and SCAD Poisson Family and Line-search Procedure Acknowledgements My mentors gave me many detailed suggestions on statistics and coding, when I encountered obstacles and submitted my work. Their knowledge and working attitude have been encouraging me to do a better job. I am very grateful to work with them through this summer. Apart from my mentors’ guidance, I think sgdnet itself also helps me a lot, since sgdnet is a well designed and clearly documented R package. And finaly, it is my fortune to have this opportunity from the R Project for Statistical Computing and Google. I hope to continue to work for sgdnet package. "],
["new-saga-features.html", "2 New SAGA Features 2.1 C-SAGA 2.2 Mini-batch SAGA 2.3 Simulated data 2.4 Observation weight", " 2 New SAGA Features The incremental gradient method SAGA (Defazio, Bach, and Lacoste-Julien 2014) used in sgdnet now supports two recently proposed methods for performance enhancement, they are C-SAGA (Park and Ryu 2018) and mini-batch SAGA (Gazagnadou, Gower, and Salmon 2019), as well as adding observation weights. 2.1 C-SAGA C-SAGA has the same algorithmic structure as SAGA, except for that the choice of gradient is cyclic rather than random, which could benefit from cache locality for a certain size of feature matrix. Considering the problem size and cache size, user can enable it by setting cyclic = TRUE in sgdnet() function, which is FALSE by default. 2.2 Mini-batch SAGA The JackSketch version of b-nice SAGA will sample a fresh batch without replacement at each iteration. Then use this batch of gradients to update the estimation of average gradient and the gradient table. Compared with original saga, we could have a look at mini-batch’s implementation in sgdnet y response vector of size n X a columnwsie p by n feature matrix B batch size beta vector of coefficients of size p g_avg gradient average max_iter maximum number of outer iterations eta the step size epoch = floor(n_samples/B) for i in 1:max_iter # a index matrix of B by epoch size ind &lt;- Ind(n_sample, B) for k in 1:epoch X_batch &lt;- X[ ,ind[,k]] # Compute a matrix of conditional mean given x_batch E[X] &lt;- DotProduct(beta, X_batch) # Compute a matrix of new gradient g_batch &lt;- Gradient(E[X], y) g_change &lt;- g_memory[ ,ind[,k]] - g_batch # Update gradient table g_memory[ ,ind[,k]] &lt;- g_batch # The gradient average step step &lt;- Step(g_change, X_batch) beta &lt;- beta - (step/B)*eta - eta*g_avg # apply penalty Penalty(beta, eta) # Update gradient average g_avg &lt;- g_avg + rowSums(g_change)/n_samples # Check convergence if (MaxChange(beta)/Max(beta) &lt; thresh) stop # algorithm as converged To use mini-batch version of saga, user could set batchsize variable to any integer that less than sample size in sgdnet() function, which is set to 1 by default. 2.3 Simulated data Here is a benchmark (R Sript) with artificial data of size 100 to illustrate C-SAGA and mini-batch SAGA’s features, where the batch size is fixed to 10 and \\(\\lambda = 0.01\\). The C-SAGA is more efficient at evaluating each epoch. 2.4 Observation weight The regularized generalized linear model with observation weights is of the type \\[ \\min_{\\beta_0, \\beta} \\left\\{-\\frac1n \\mathcal{L}\\left(\\beta_0,\\beta; \\mathbf{y}, \\mathbf{W} \\mathbf{X}\\right)+ h_{\\lambda}(\\beta) \\right \\}, \\] where \\(\\mathbf{W}\\) is a \\(n\\) by \\(n\\) diagonal matrix with weight \\(w_i\\) for \\(x_i\\) along its diagnoal, and \\(h_{\\lambda}(\\beta)\\) is the penalty function with \\(\\lambda\\) as the regularization strength. In sgdnet, user can input a vector of size n for \\(\\mathbf{W}\\), whose sum should be \\(n\\). sgdnet(x, y, weights = c(0.5, 1.5, rep(1, n-2))) Reference "],
["optimize-mini-batching.html", "3 Optimize Mini-Batching 3.1 Optimal batch size and step size for ridge 3.2 Benchmark", " 3 Optimize Mini-Batching The enhancement for SAGA performance by mini-batching depends on two things. Firstly, mini-batching is expected to benefit from the efficiency of vectorized computation, which reduces the cost of a single epoch. Secondly, while the total complexity of SAGA decreases linearly in batch size up to a certain value, emperical study also shows that the complexity explodes with values greater than an optimal mini-batch size (Gazagnadou, Gower, and Salmon 2019). Therefore, we expect a batch size at the optimal point with vectorized computation to bring us the best performance enhancement. 3.1 Optimal batch size and step size for ridge For ridge regression, the SAGA step size \\(\\eta\\) proposed by Defazio is defined as \\(\\eta_{\\text{Defazio}} = \\frac{1}{3(\\mu n + L_{\\text{max}})}\\) (Defazio, Bach, and Lacoste-Julien 2014), where the maximum smoothness constant \\(L_{\\text{max}}\\) is bound by the largest sample-wise squared norm of the feature matrix. The optimal \\(\\eta_{\\text{optimal}}\\) and batch size \\(\\mathcal{B}^*\\) for SAGA proposed by Gazagnadou are: \\[\\begin{align} \\mathcal{B}^* &amp;= \\left \\lfloor 1 + \\frac{\\mu (n-1)}{4 (\\overline{L} + \\lambda)} \\right \\rfloor \\\\ \\eta_{\\text{optimal}} &amp;= \\frac{1}{4} \\frac{1}{\\text{max} \\Big \\{ \\mathcal{L}(b) + \\lambda, \\frac{1}{b} \\frac{n-b}{n-1} L_{\\text{max}} + \\frac{\\mu n}{4 b} \\Big \\}} \\\\ \\mathcal{L}(b) &amp;= \\frac{n(b - 1)}{b(n-1)} L + \\frac{n - b}{b(n-1)} L_{\\text{max}} \\end{align}\\] where the ridge strength \\(\\lambda\\) has been added to every smoothness constant. If \\(L\\), the largest eigenvalue of \\(\\mathbf{X}^T \\mathbf{X}\\) is significantly larger than the rest of eigenvalues, then the mean smoothness constant \\(\\overline{L}\\) is replaced by \\(L\\) (Gazagnadou, Gower, and Salmon 2019). The objective function is \\(\\mu\\)-strongly convex, and we use the ridge strength for \\(\\mu\\). Use ridge regression with abalone dataset (\\(n = 4177, \\,\\, p = 9\\)) as an example (\\(\\lambda = 10\\)) (R Sript). Under \\(\\eta_{\\text{Defazio}}\\), the complexity grows with \\(b\\), while with \\(\\eta_{\\text{optimal}}\\), it decreases to a lower level. The red line indicates \\(\\mathcal{B}^*\\) for this problem. We could have a look at \\(\\mathcal{B}^*\\) and \\(\\eta_{\\text{optimal}}\\) along abalone’s regularization path. 3.2 Benchmark This benchmark (R Sript) was run on Cedar of Canada Compute. It is a comparison of the returned epoch loss and elapsed time of one run between \\((b = 1, \\eta_{\\text{Defazio}})\\) and \\((b = \\mathcal{B}^*, \\eta_{\\text{optimal}})\\), by setting options(sgdnet.debug = TRUE). These datasets is publicly accessible, they are the testing set of YearPredictionMSD (\\(n = 5,1630, \\,\\, p = 90\\)), scaled version of covtype.binary (\\(n = 58,1012, \\,\\, p = 54\\)), ijcnn1 (\\(n = 14,1691, \\,\\, p = 22\\)) from LIBSVM and unscaled version of slice (\\(n = 5,3500, \\,\\, p = 384\\)) from UCI. Gaussian data with ridge Binomial data with ridge With \\((b = \\mathcal{B}^*, \\eta_{\\text{optimal}})\\), mini-batch SAGA could converge with less epoch for most examples, which brings hope to futrue work based on Eigen 3.4’s coming functionality of geting view into submatrix by indexing with a vector of indices. Reference "],
["nonconvex-penalty.html", "4 Nonconvex Penalty 4.1 Introduction 4.2 MCP 4.3 SCAD", " 4 Nonconvex Penalty 4.1 Introduction In addition to lasso, ridge and elastic net penalty, sgdnet also fits along regularization path for gaussian regression using nonconvex penalties, which are minimax concave penalty (MCP) (Zhang 2010) and smoothly clipped absolute penalty (SCAD) (Fan and Li 2001). The ridge part in sgdnet use scale update \\(\\beta^{k + n} = \\beta^{k} \\prod _{i = 1}^{n}(1- \\eta(1-\\alpha) \\lambda)\\), where \\(\\lambda\\) is the regularization strength, \\(\\alpha\\) is elastic net mixing parameter and \\(\\eta\\) is the step size. Other penalties are applied via a proximal step at each iteration from the coefficient \\(\\beta^{k+\\frac{1}{2}}\\) after the average gradient step. The proximal step solves a minimization problem: \\(\\begin{aligned} \\beta^{k + \\frac{1}{2}} \\leftarrow &amp; \\, \\beta^{k} - \\eta \\nabla \\\\ \\beta^{k} \\leftarrow &amp; \\, \\underset{\\beta}{argmin} \\Big( h_{\\lambda}(\\beta) + \\frac{1}{2 \\eta} \\left \\| \\beta - \\beta^{k + \\frac{1}{2}} \\right \\|_2^2 \\Big) \\end{aligned}\\) where \\(h_{\\lambda}(\\beta)\\) is the penalty function. 4.2 MCP The definition for minimax concave penalty for each element of coefficient \\(\\beta\\) is given by: \\[\\begin{equation} p_{\\lambda, \\gamma}(\\beta_j) = \\begin{cases} \\lambda \\beta_j - \\frac{\\beta_j^2}{2 \\gamma} &amp; \\text{if} \\,\\, |\\beta_j| \\leq \\gamma \\lambda \\\\ \\frac{1}{2} \\gamma \\lambda^2 &amp; \\text{otherwise.} \\end{cases}\\end{equation}\\] where \\(\\lambda \\geq 0\\) is the regularization strength and \\(\\gamma &gt; 1\\) is a user defined nonconvexity parameter. We have \\(h_{\\lambda}(\\beta) = p_{\\alpha \\lambda, \\gamma}(\\beta) + \\frac{(1 - \\alpha)\\lambda}{2} ||\\beta||_2^2\\) if user wants to mix ridge with MCP in the proximal step by setting \\(\\alpha\\) (B. Huang Jian. and Zhang 2016). The proximal operator for them is given by: \\[\\begin{equation} \\beta_j \\leftarrow \\begin{cases} \\frac{ \\Big (1 - \\eta \\alpha \\lambda \\Big )_{+} \\beta_j}{1 + \\eta (1-\\alpha)\\lambda-\\frac{\\eta}{\\gamma}} &amp; \\text{if} \\,\\, |\\beta_j| \\leq \\gamma \\alpha \\lambda \\Big (1 + (1- \\alpha) \\lambda\\Big ) \\\\ \\frac{\\beta_j}{1+\\eta(1-\\alpha)\\lambda} &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] MCP begins by applying the same rate of penalization as lasso, but continuously relaxes that penalization until \\(\\beta_j\\) becomes large (Breheny and Huang 2011). When \\(\\alpha = 0\\), this becomes the proximal updates for ridge (Friedman, Hastie, and Tibshirani 2010). For MCP, sgdnet set the default noncovexity parameter \\(\\gamma\\) to \\(3\\). Here we use the example from ncvreg to demostrate its usage. data(prostate) x &lt;- prostate$x y &lt;- prostate$y sgd_mcp &lt;- sgdnet(x, y, non_convexity = 3, penalty = &quot;MCP&quot;) ncv_mcp &lt;- ncvreg(x, y, gamma = 3, penalty = &quot;MCP&quot;) sgd_mnet &lt;- sgdnet(x, y, non_convexity = 3, alpha = 0.5, penalty = &quot;MCP&quot;) ncv_mnet &lt;- ncvreg(x, y, gamma = 3, alpha = 0.5, penalty = &quot;MCP&quot;) MCP/Mnet penalized Gaussian model on prostate dataset 4.3 SCAD Following the same notation, the definition for smoothly clipped absolute penalty for each element of coefficient \\(\\beta\\) is given by: \\[\\begin{equation} p_{\\lambda, \\gamma}(\\beta_j) = \\begin{cases} \\lambda \\beta_j &amp; \\text{if} \\, \\,|\\beta_j| \\leq \\lambda\\\\ \\frac{\\gamma \\lambda \\beta_j - \\frac{1}{2} (\\beta_j^2 + \\lambda^2)}{\\gamma - 1} \\gamma \\lambda^2 &amp; \\text{if} \\,\\, \\lambda &lt; |\\beta_j| \\leq \\gamma \\lambda \\\\ \\frac{\\lambda^2 (\\gamma^2 - 1)}{2(\\gamma - 1)} &amp; \\text{otherwise.} \\end{cases} \\end{equation}\\] where \\(\\lambda \\geq 0\\) and \\(\\gamma &gt; 2\\). The proximal operator for SCAD (Wang 2016) of strength \\(\\alpha \\lambda\\), with ridge of strength \\((1-\\alpha)\\lambda\\) can be solved as: \\[\\begin{equation} \\beta_j \\leftarrow \\begin{cases} \\frac{\\Big ( 1-\\eta \\alpha \\lambda \\Big )_{+} \\beta_j}{1+\\eta(1-\\alpha)\\lambda} &amp; \\text{if} \\, \\,|\\beta_j| \\leq \\alpha \\lambda + \\eta \\alpha \\lambda ( 1 + (1-\\alpha) \\lambda)\\\\ \\frac{\\Big ( 1 - \\frac{\\gamma}{ \\gamma - 1} \\eta \\alpha \\lambda \\Big)_{+} \\beta_j}{1- \\frac{\\eta}{\\gamma - 1} + \\eta (1- \\alpha) \\lambda} &amp; \\text{if} \\,\\, \\alpha \\lambda + \\eta \\alpha \\lambda ( 1 + (1-\\alpha) \\lambda) &lt; |\\beta_j| \\leq \\gamma \\alpha \\lambda ( 1 + \\eta (1 - \\alpha) \\lambda) \\\\ \\frac{\\beta_j}{1+\\eta(1-\\alpha)\\lambda} &amp; \\text{otherwise.} \\end{cases} \\end{equation}\\] Similar to MCP, SCAD begins by applying lasso’s penalization rate, and reduces the rate to 0 as \\(\\beta_j\\) gets away from 0. The difference is in the way to make this transition. sgdnet has a default nonconvexity parameter \\(\\gamma = 3.7\\). It can be specified by the non_convexity variable in sgdnet() function. sgd_scad &lt;- sgdnet(x, y, non_convexity = 3.7, penalty = &quot;SCAD&quot;) ncv_scad &lt;- ncvreg(x, y, gamma = 3.7, penalty = &quot;SCAD&quot;) sgd_snet &lt;- sgdnet(x, y, non_convexity = 3.7, alpha = 0.5, penalty = &quot;SCAD&quot;) ncv_snet &lt;- ncvreg(x, y, gamma = 3.7, alpha = 0.5, penalty = &quot;SCAD&quot;) SCAD/Snet penalized Gaussian model on prostate dataset Reference "],
["poisson-models.html", "5 Poisson Models 5.1 Overview 5.2 Example", " 5 Poisson Models 5.1 Overview Poisson regression via sgdnet() is now supported by sgdnet, which has the following objective function in elastic net regularization: \\[ -\\frac1n \\sum_{i=1}^n \\Big( y_i (\\beta_0 + \\beta^\\mathsf{T} x_i) - e^{\\beta_0 + \\beta^\\mathsf{T} x_i} \\Big) + \\lambda \\left( \\frac{1 - \\alpha}{2} ||\\beta||_2^2 + \\alpha||\\beta||_1 \\right) \\] We use the deviance function for a Poisson distribution \\(y_i \\sim Pois(\\mu_i)\\) to evaluate model loss: \\[ 2 \\sum_{i=1}^n \\Big [ y_i log \\, (\\frac{y_i}{\\mu_i}) - (y_i - \\mu_i) \\Big ] \\] where \\(\\mu_i = e^{\\beta_0 + \\beta^\\mathsf{T} x_i}\\). The Poisson loss function is convex but not Lipschitz-continuous. To guarantee SAGA’s convergence, which rely on derivatives’ Lipschitz-continuity, we will update the approximated Lipschitz constant \\(L\\) while running the algorithm. Starting with an initial estimate \\(L^{0}\\) as \\(L_{\\text{max}}\\), which is the largest sample-wise squared norm of the feature matrix, an invalid \\(L^{k}\\) will be doubled after evaluating \\(f_i\\) with the coefficient \\(\\beta_{k}\\) at iteration \\(k\\), if the following inequality is not satisfied (Schmidt, Le Roux, and Bach 2017): \\[ f_{i_{k}} \\Big ( (\\beta_{0k} + \\beta_{k}^\\mathsf{T} x_i) - \\frac{f^{&#39;}_{i_{k}}(\\beta_{0k},\\beta_{k})}{L^{k}} \\left \\| x_i \\right \\|^2_{2} \\Big ) \\leq f_{i_{k}} (\\beta_{0k} + \\beta_{k}^\\mathsf{T} x_i) - \\frac{1}{2L^{k}} \\left \\| f^{&#39;}_{i_{k}}(\\beta_{0k}, \\beta_{k}) \\right \\|^2 \\left \\| x_i \\right \\|^2_{2} \\] where the individual norm is precomputed. The corresponding cross-validation method via cv_sgdnet(), as well as making prediction by predict() and model performance measure by score() are also avilable for Poisson in sgdnet. 5.2 Example For illustration, we compare with glmnet using the fitting results of caddisfly dataset along regularization path. This dataset describes the abundances of one of 17 species in original dataset, which is Stactobiella risi, and 12 meteorological features that may influence its presence during 49 trapping night. data(&quot;caddisfly&quot;) x &lt;- caddisfly$x y &lt;- caddisfly$y sfit0 &lt;- sgdnet(x, y, family = &quot;poisson&quot;, alpha = 0) gfit0 &lt;- glmnet(x, y, family = &quot;poisson&quot;, alpha = 0) Poisson model with ridge regression on caddisfly dataset sfit1 &lt;- sgdnet(x, y, family = &quot;poisson&quot;, alpha = 1) gfit1 &lt;- glmnet(x, y, family = &quot;poisson&quot;, alpha = 1) Poisson model with lasso regression on caddisfly dataset Reference "],
["reference.html", "6 Reference", " 6 Reference "]
]
